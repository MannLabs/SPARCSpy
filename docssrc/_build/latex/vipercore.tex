%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsable pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Viper Core}
\date{Oct 13, 2021}
\release{}
\author{Georg Wallmann, Sophia Mädler, Niklas Schmacke}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{index:id1}}{\hyperref[\detokenize{index:welcome-to-viper-core-s-documentation}]{\sphinxcrossref{Welcome to Viper Core’s documentation!}}}

\end{itemize}
\end{sphinxShadowBox}

\sphinxAtStartPar
Welcome to the show


\chapter{command line arguments}
\label{\detokenize{pages/vipercmd:command-line-arguments}}\label{\detokenize{pages/vipercmd::doc}}

\section{viper\sphinxhyphen{}split}
\label{\detokenize{pages/vipercmd:viper-split}}
\sphinxAtStartPar

\sphinxAtStartPar
Manipulate existing single cell hdf5 datasets.


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{viper}\PYG{o}{\PYGZhy{}}\PYG{n}{split} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{OUTPUT} \PYG{n}{OUTPUT}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{r}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{t} \PYG{n}{THREADS}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{c}\PYG{p}{]} \PYG{n}{input\PYGZus{}dataset}
\end{sphinxVerbatim}


\subsection{Positional Arguments}
\label{\detokenize{pages/vipercmd:Positional Arguments}}\begin{optionlist}{3cm}
\item [input\_dataset]  
\sphinxAtStartPar
input dataset which should be split
\end{optionlist}


\subsection{Named Arguments}
\label{\detokenize{pages/vipercmd:Named Arguments}}\begin{optionlist}{3cm}
\item [\sphinxhyphen{}o, \sphinxhyphen{}\sphinxhyphen{}output]  
\sphinxAtStartPar
Output definition \textless{}name\textgreater{} \textless{}length\textgreater{}. For example \sphinxhyphen{}o test.h5 0.9 or or \sphinxhyphen{}o test.h5 1000. If the sum of all lengths is \textless{}= 1, it is interpretated as fraction. Else its used as absolute value
\item [\sphinxhyphen{}r, \sphinxhyphen{}\sphinxhyphen{}random]  
\sphinxAtStartPar
shuffle single cells randomly

\sphinxAtStartPar
Default: False
\item [\sphinxhyphen{}t, \sphinxhyphen{}\sphinxhyphen{}threads]  
\sphinxAtStartPar
number of threads

\sphinxAtStartPar
Default: 4
\item [\sphinxhyphen{}c, \sphinxhyphen{}\sphinxhyphen{}compression]  
\sphinxAtStartPar
use lzf compression

\sphinxAtStartPar
Default: False
\end{optionlist}

\sphinxAtStartPar
Manipulate existing single cell hdf5 datasets
can be used for splitting, shuffleing and compression / decompression

\sphinxAtStartPar
Splitting with shuffle and compression:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{viper}\PYG{o}{\PYGZhy{}}\PYG{n}{split} \PYG{n}{single\PYGZus{}cells}\PYG{o}{.}\PYG{n}{h5} \PYG{o}{\PYGZhy{}}\PYG{n}{r} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{train}\PYG{o}{.}\PYG{n}{h5} \PYG{l+m+mf}{0.9} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{test}\PYG{o}{.}\PYG{n}{h5} \PYG{l+m+mf}{0.05} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{validate}\PYG{o}{.}\PYG{n}{h5} \PYG{l+m+mf}{0.05}
\end{sphinxVerbatim}

\sphinxAtStartPar
Shuffle:
viper\sphinxhyphen{}split single\_cells.h5 \sphinxhyphen{}r \sphinxhyphen{}o single\_cells.h5 1.0

\sphinxAtStartPar
Compression:
viper\sphinxhyphen{}split single\_cells.h5 \sphinxhyphen{}c \sphinxhyphen{}o single\_cells.h5 1.0

\sphinxAtStartPar
Decompression:
viper\sphinxhyphen{}split single\_cells.h5 \sphinxhyphen{}o single\_cells.h5 1.0


\section{viper\sphinxhyphen{}stat}
\label{\detokenize{pages/vipercmd:viper-stat}}
\sphinxAtStartPar

\sphinxAtStartPar
Scan directory for viper projects.


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{viper}\PYG{o}{\PYGZhy{}}\PYG{n}{stat} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{t} \PYG{n}{THREADS}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{r} \PYG{n}{RECURSION}\PYG{p}{]} \PYG{p}{[}\PYG{n}{search\PYGZus{}directory}\PYG{p}{]}
\end{sphinxVerbatim}


\subsection{Positional Arguments}
\label{\detokenize{pages/vipercmd:Positional Arguments}}\begin{optionlist}{3cm}
\item [search\_directory]  
\sphinxAtStartPar
directory containing viper projects
\end{optionlist}


\subsection{Named Arguments}
\label{\detokenize{pages/vipercmd:Named Arguments}}\begin{optionlist}{3cm}
\item [\sphinxhyphen{}t, \sphinxhyphen{}\sphinxhyphen{}threads]  
\sphinxAtStartPar
number of threads

\sphinxAtStartPar
Default: 8
\item [\sphinxhyphen{}r, \sphinxhyphen{}\sphinxhyphen{}recursion]  
\sphinxAtStartPar
levels of recursion

\sphinxAtStartPar
Default: 5
\end{optionlist}


\chapter{ml}
\label{\detokenize{pages/ml:ml}}\label{\detokenize{pages/ml::doc}}

\section{datasets}
\label{\detokenize{pages/ml:module-vipercore.ml.datasets}}\label{\detokenize{pages/ml:datasets}}\index{module@\spxentry{module}!vipercore.ml.datasets@\spxentry{vipercore.ml.datasets}}\index{vipercore.ml.datasets@\spxentry{vipercore.ml.datasets}!module@\spxentry{module}}\index{HDF5SingleCellDataset (class in vipercore.ml.datasets)@\spxentry{HDF5SingleCellDataset}\spxextra{class in vipercore.ml.datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.datasets.HDF5SingleCellDataset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.datasets.}}\sphinxbfcode{\sphinxupquote{HDF5SingleCellDataset}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}\DUrole{p}{:} \DUrole{n}{Any}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}\DUrole{p}{:} \DUrole{n}{Any}}}{}
\end{fulllineitems}

\index{NPYSingleCellDataset (class in vipercore.ml.datasets)@\spxentry{NPYSingleCellDataset}\spxextra{class in vipercore.ml.datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.datasets.NPYSingleCellDataset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.datasets.}}\sphinxbfcode{\sphinxupquote{NPYSingleCellDataset}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}\DUrole{p}{:} \DUrole{n}{Any}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}\DUrole{p}{:} \DUrole{n}{Any}}}{}
\sphinxAtStartPar
Summary line.

\sphinxAtStartPar
Extended description of function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{arg1}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Description of arg1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{arg2}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Description of arg2

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Description of return value

\item[{Return type}] \leavevmode
\sphinxAtStartPar
bool

\end{description}\end{quote}

\end{fulllineitems}



\section{metrics}
\label{\detokenize{pages/ml:module-vipercore.ml.metrics}}\label{\detokenize{pages/ml:metrics}}\index{module@\spxentry{module}!vipercore.ml.metrics@\spxentry{vipercore.ml.metrics}}\index{vipercore.ml.metrics@\spxentry{vipercore.ml.metrics}!module@\spxentry{module}}\index{auc() (in module vipercore.ml.metrics)@\spxentry{auc()}\spxextra{in module vipercore.ml.metrics}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.metrics.auc}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{vipercore.ml.metrics.}}\sphinxbfcode{\sphinxupquote{auc}}}{\emph{\DUrole{n}{predictions}}, \emph{\DUrole{n}{labels}}}{}
\sphinxAtStartPar
area under curve of the receiver operator characteristic

\end{fulllineitems}

\index{precision() (in module vipercore.ml.metrics)@\spxentry{precision()}\spxextra{in module vipercore.ml.metrics}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.metrics.precision}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{vipercore.ml.metrics.}}\sphinxbfcode{\sphinxupquote{precision}}}{\emph{\DUrole{n}{predictions}}, \emph{\DUrole{n}{labels}}, \emph{\DUrole{n}{pos\_label}\DUrole{o}{=}\DUrole{default_value}{0}}}{}
\sphinxAtStartPar
precision for predictiong class pos\_label

\end{fulllineitems}

\index{precision\_top\_n() (in module vipercore.ml.metrics)@\spxentry{precision\_top\_n()}\spxextra{in module vipercore.ml.metrics}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.metrics.precision_top_n}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{vipercore.ml.metrics.}}\sphinxbfcode{\sphinxupquote{precision\_top\_n}}}{\emph{\DUrole{n}{predictions}}, \emph{\DUrole{n}{labels}}, \emph{\DUrole{n}{pos\_label}\DUrole{o}{=}\DUrole{default_value}{0}}, \emph{\DUrole{n}{top\_n}\DUrole{o}{=}\DUrole{default_value}{0.01}}}{}
\sphinxAtStartPar
precision for the top\_n percentage (0.01 = 1\%) predictions

\end{fulllineitems}

\index{recall() (in module vipercore.ml.metrics)@\spxentry{recall()}\spxextra{in module vipercore.ml.metrics}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.metrics.recall}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{vipercore.ml.metrics.}}\sphinxbfcode{\sphinxupquote{recall}}}{\emph{\DUrole{n}{predictions}}, \emph{\DUrole{n}{labels}}, \emph{\DUrole{n}{pos\_label}\DUrole{o}{=}\DUrole{default_value}{0}}}{}
\sphinxAtStartPar
recall for predictiong class pos\_label

\end{fulllineitems}

\index{recall\_top\_n() (in module vipercore.ml.metrics)@\spxentry{recall\_top\_n()}\spxextra{in module vipercore.ml.metrics}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.metrics.recall_top_n}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{vipercore.ml.metrics.}}\sphinxbfcode{\sphinxupquote{recall\_top\_n}}}{\emph{\DUrole{n}{predictions}}, \emph{\DUrole{n}{labels}}, \emph{\DUrole{n}{pos\_label}\DUrole{o}{=}\DUrole{default_value}{0}}, \emph{\DUrole{n}{top\_n}\DUrole{o}{=}\DUrole{default_value}{0.01}}}{}
\sphinxAtStartPar
recall for the top\_n percentage (0.01 = 1\%) predictions

\end{fulllineitems}



\section{models}
\label{\detokenize{pages/ml:module-vipercore.ml.models}}\label{\detokenize{pages/ml:models}}\index{module@\spxentry{module}!vipercore.ml.models@\spxentry{vipercore.ml.models}}\index{vipercore.ml.models@\spxentry{vipercore.ml.models}!module@\spxentry{module}}\index{GolgiCAE (class in vipercore.ml.models)@\spxentry{GolgiCAE}\spxextra{class in vipercore.ml.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiCAE}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.models.}}\sphinxbfcode{\sphinxupquote{GolgiCAE}}}{\emph{\DUrole{n}{cfg}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}B\textquotesingle{}}}, \emph{\DUrole{n}{in\_channels}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{out\_channels}\DUrole{o}{=}\DUrole{default_value}{5}}}{}~\index{forward() (vipercore.ml.models.GolgiCAE method)@\spxentry{forward()}\spxextra{vipercore.ml.models.GolgiCAE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiCAE.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
\sphinxAtStartPar
Defines the computation performed at every call.

\sphinxAtStartPar
Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{GolgiVAE (class in vipercore.ml.models)@\spxentry{GolgiVAE}\spxextra{class in vipercore.ml.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVAE}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.models.}}\sphinxbfcode{\sphinxupquote{GolgiVAE}}}{\emph{\DUrole{n}{in\_channels}}, \emph{\DUrole{n}{out\_channels}}, \emph{\DUrole{n}{latent\_dim}}, \emph{\DUrole{n}{hidden\_dims}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{decode() (vipercore.ml.models.GolgiVAE method)@\spxentry{decode()}\spxextra{vipercore.ml.models.GolgiVAE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVAE.decode}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{decode}}}{\emph{\DUrole{n}{z}}}{}
\sphinxAtStartPar
Maps the given latent codes
onto the image space.
:param z: (Tensor) {[}B x D{]}
:return: (Tensor) {[}B x C x H x W{]}

\end{fulllineitems}

\index{encode() (vipercore.ml.models.GolgiVAE method)@\spxentry{encode()}\spxextra{vipercore.ml.models.GolgiVAE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVAE.encode}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{encode}}}{\emph{\DUrole{n}{input}}}{}
\sphinxAtStartPar
Encodes the input by passing through the encoder network
and returns the latent codes.
:param input: (Tensor) Input tensor to encoder {[}N x C x H x W{]}
:return: (Tensor) List of latent codes

\end{fulllineitems}

\index{forward() (vipercore.ml.models.GolgiVAE method)@\spxentry{forward()}\spxextra{vipercore.ml.models.GolgiVAE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVAE.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{input}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Defines the computation performed at every call.

\sphinxAtStartPar
Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{loss\_function() (vipercore.ml.models.GolgiVAE method)@\spxentry{loss\_function()}\spxextra{vipercore.ml.models.GolgiVAE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVAE.loss_function}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss\_function}}}{\emph{\DUrole{n}{target}}, \emph{\DUrole{n}{output}}, \emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\begin{quote}

\sphinxAtStartPar
Computes the VAE loss function.
KL(N(mu, sigma), N(0, 1)) = log
\end{quote}

\sphinxAtStartPar
rac\{1\}\{sigma\} +
rac\{sigma\textasciicircum{}2 + mu\textasciicircum{}2\}\{2\} \sphinxhyphen{}
rac\{1\}\{2\}
\begin{quote}
\begin{quote}\begin{description}
\item[{param args}] \leavevmode
\item[{param kwargs}] \leavevmode
\item[{return}] \leavevmode
\end{description}\end{quote}
\end{quote}

\end{fulllineitems}

\index{reparameterize() (vipercore.ml.models.GolgiVAE method)@\spxentry{reparameterize()}\spxextra{vipercore.ml.models.GolgiVAE method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVAE.reparameterize}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reparameterize}}}{\emph{\DUrole{n}{mu}}, \emph{\DUrole{n}{logvar}}}{}
\sphinxAtStartPar
Reparameterization trick to sample from N(mu, var) from
N(0,1).
:param mu: (Tensor) Mean of the latent Gaussian {[}B x D{]}
:param logvar: (Tensor) Standard deviation of the latent Gaussian {[}B x D{]}
:return: (Tensor) {[}B x D{]}

\end{fulllineitems}


\end{fulllineitems}

\index{GolgiVGG (class in vipercore.ml.models)@\spxentry{GolgiVGG}\spxextra{class in vipercore.ml.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVGG}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.models.}}\sphinxbfcode{\sphinxupquote{GolgiVGG}}}{\emph{\DUrole{n}{cfg}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}B\textquotesingle{}}}, \emph{\DUrole{n}{dimensions}\DUrole{o}{=}\DUrole{default_value}{196}}, \emph{\DUrole{n}{in\_channels}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{num\_classes}\DUrole{o}{=}\DUrole{default_value}{2}}}{}~\index{forward() (vipercore.ml.models.GolgiVGG method)@\spxentry{forward()}\spxextra{vipercore.ml.models.GolgiVGG method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.models.GolgiVGG.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
\sphinxAtStartPar
Defines the computation performed at every call.

\sphinxAtStartPar
Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\section{plmodels}
\label{\detokenize{pages/ml:module-vipercore.ml.plmodels}}\label{\detokenize{pages/ml:plmodels}}\index{module@\spxentry{module}!vipercore.ml.plmodels@\spxentry{vipercore.ml.plmodels}}\index{vipercore.ml.plmodels@\spxentry{vipercore.ml.plmodels}!module@\spxentry{module}}\index{AEModel (class in vipercore.ml.plmodels)@\spxentry{AEModel}\spxextra{class in vipercore.ml.plmodels}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.plmodels.}}\sphinxbfcode{\sphinxupquote{AEModel}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{hparams}}}{}~\index{configure\_optimizers() (vipercore.ml.plmodels.AEModel method)@\spxentry{configure\_optimizers()}\spxextra{vipercore.ml.plmodels.AEModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.configure_optimizers}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{configure\_optimizers}}}{}{}
\sphinxAtStartPar
Choose what optimizers and learning\sphinxhyphen{}rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of these 6 options.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single optimizer}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{List or Tuple} of optimizers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two lists} \sphinxhyphen{} The first list has multiple optimizers, and the second has multiple LR schedulers (or
multiple lr\_dict).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dictionary}, with an \sphinxcode{\sphinxupquote{"optimizer"}} key, and (optionally) a \sphinxcode{\sphinxupquote{"lr\_scheduler"}}
key whose value is a single LR scheduler or lr\_dict.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tuple of dictionaries} as described above, with an optional \sphinxcode{\sphinxupquote{"frequency"}} key.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{None} \sphinxhyphen{} Fit will run without any optimizer.

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The lr\_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lr\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{lr\PYGZus{}scheduler}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The LR scheduler instance (required)}
    \PYG{c+c1}{\PYGZsh{} The unit of the scheduler\PYGZsq{}s step size, could also be \PYGZsq{}step\PYGZsq{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epoch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The frequency of the scheduler}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{monitor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Metric for `ReduceLROnPlateau` to monitor}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Whether to crash the training if `monitor` is not found}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Custom name for `LearningRateMonitor` to use}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Only the \sphinxcode{\sphinxupquote{"scheduler"}} key is required, the rest will be set to the defaults above.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{frequency}} value specified in a dict along with the \sphinxcode{\sphinxupquote{optimizer}} key is an int corresponding
to the number of sequential batches optimized with the specific optimizer.
It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.
This is different from the \sphinxcode{\sphinxupquote{frequency}} value specified in the lr\_dict mentioned below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{optimizer\PYGZus{}one} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{optimizer\PYGZus{}two} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}one}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}two}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the \sphinxcode{\sphinxupquote{lr\_scheduler}} key in the above dict,
the scheduler will only be updated when its optimizer is being used.
\end{sphinxadmonition}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} most cases}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} multiple optimizer case (e.g.: GAN)}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}

\PYG{c+c1}{\PYGZsh{} example with learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with step\PYGZhy{}based learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{gen\PYGZus{}sch} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ExponentialLR}\PYG{p}{(}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+m+mf}{0.99}\PYG{p}{)}\PYG{p}{,}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} called after each training step}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} called every epoch}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{gen\PYGZus{}sch}\PYG{p}{,} \PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with optimizer frequencies}
\PYG{c+c1}{\PYGZsh{} see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1}
\PYG{c+c1}{\PYGZsh{} https://arxiv.org/abs/1704.00028}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}critic} \PYG{o}{=} \PYG{l+m+mi}{5}
    \PYG{k}{return} \PYG{p}{(}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{n\PYGZus{}critic}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Some things to know:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lightning calls \sphinxcode{\sphinxupquote{.backward()}} and \sphinxcode{\sphinxupquote{.step()}} on each optimizer and learning rate scheduler as needed.

\item {} 
\sphinxAtStartPar
If you use 16\sphinxhyphen{}bit precision (\sphinxcode{\sphinxupquote{precision=16}}), Lightning will automatically handle the optimizers.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.training_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{training\_step()}}}}} will have an additional \sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\item {} 
\sphinxAtStartPar
If you use \sphinxcode{\sphinxupquote{torch.optim.LBFGS}}, Lightning handles the closure function automatically for you.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
at each training step.

\item {} 
\sphinxAtStartPar
If you need to control how often those optimizers step or override the default \sphinxcode{\sphinxupquote{.step()}} schedule,
override the \sphinxcode{\sphinxupquote{optimizer\_step()}} hook.

\end{itemize}
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (vipercore.ml.plmodels.AEModel method)@\spxentry{forward()}\spxextra{vipercore.ml.plmodels.AEModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
\sphinxAtStartPar
Same as \sphinxcode{\sphinxupquote{torch.nn.Module.forward()}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{*args}} \textendash{} Whatever you decide to pass into the forward method.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} \textendash{} Keyword arguments are also possible.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Your model’s output

\end{description}\end{quote}

\end{fulllineitems}

\index{on\_train\_epoch\_end() (vipercore.ml.plmodels.AEModel method)@\spxentry{on\_train\_epoch\_end()}\spxextra{vipercore.ml.plmodels.AEModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.on_train_epoch_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_end}}}{\emph{\DUrole{n}{outputs}}}{}
\sphinxAtStartPar
Called in the training loop at the very end of the epoch.

\sphinxAtStartPar
To access all batch outputs at the end of the epoch, either:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Implement \sphinxtitleref{training\_epoch\_end} in the LightningModule OR

\item {} 
\sphinxAtStartPar
Cache data across steps on the attribute(s) of the \sphinxtitleref{LightningModule} and access them in this hook

\end{enumerate}

\end{fulllineitems}

\index{on\_train\_epoch\_start() (vipercore.ml.plmodels.AEModel method)@\spxentry{on\_train\_epoch\_start()}\spxextra{vipercore.ml.plmodels.AEModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.on_train_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the training loop at the very beginning of the epoch.

\end{fulllineitems}

\index{on\_validation\_epoch\_start() (vipercore.ml.plmodels.AEModel method)@\spxentry{on\_validation\_epoch\_start()}\spxextra{vipercore.ml.plmodels.AEModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.on_validation_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_validation\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the validation loop at the very beginning of the epoch.

\end{fulllineitems}

\index{training\_step() (vipercore.ml.plmodels.AEModel method)@\spxentry{training\_step()}\spxextra{vipercore.ml.plmodels.AEModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AEModel.training_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Integer displaying index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{optimizer\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} When using multiple optimizers, this argument will also be present.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hiddens}} (\sphinxcode{\sphinxupquote{Tensor}}) \textendash{} Passed in if
{\color{red}\bfseries{}:paramref:\textasciigrave{}\textasciitilde{}pytorch\_lightning.core.lightning.LightningModule.truncated\_bptt\_steps\textasciigrave{}} \textgreater{} 0.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Tensor}} \sphinxhyphen{} The loss tensor

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dict}} \sphinxhyphen{} A dictionary. Can include any keys, but must include the key \sphinxcode{\sphinxupquote{\textquotesingle{}loss\textquotesingle{}}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Training will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Returning \sphinxcode{\sphinxupquote{None}} is currently not supported for multi\sphinxhyphen{}GPU or TPU, or with 16\sphinxhyphen{}bit precision enabled.
\end{sphinxadmonition}

\sphinxAtStartPar
In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{o}{=} \PYG{n}{batch}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{encoder}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you define multiple optimizers, this step will be called with an additional
\sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Multiple optimizers (e.g.: GANs)}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{optimizer\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with encoder}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with decoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Truncated back\PYGZhy{}propagation through time}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} hiddens are the hidden states from the previous truncated backprop step}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{n}{out}\PYG{p}{,} \PYG{n}{hiddens} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lstm}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hiddens}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{hiddens}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{AutoEncoderModel (class in vipercore.ml.plmodels)@\spxentry{AutoEncoderModel}\spxextra{class in vipercore.ml.plmodels}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.plmodels.}}\sphinxbfcode{\sphinxupquote{AutoEncoderModel}}}{\emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{configure\_optimizers() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{configure\_optimizers()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.configure_optimizers}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{configure\_optimizers}}}{}{}
\sphinxAtStartPar
Choose what optimizers and learning\sphinxhyphen{}rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of these 6 options.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single optimizer}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{List or Tuple} of optimizers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two lists} \sphinxhyphen{} The first list has multiple optimizers, and the second has multiple LR schedulers (or
multiple lr\_dict).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dictionary}, with an \sphinxcode{\sphinxupquote{"optimizer"}} key, and (optionally) a \sphinxcode{\sphinxupquote{"lr\_scheduler"}}
key whose value is a single LR scheduler or lr\_dict.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tuple of dictionaries} as described above, with an optional \sphinxcode{\sphinxupquote{"frequency"}} key.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{None} \sphinxhyphen{} Fit will run without any optimizer.

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The lr\_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lr\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{lr\PYGZus{}scheduler}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The LR scheduler instance (required)}
    \PYG{c+c1}{\PYGZsh{} The unit of the scheduler\PYGZsq{}s step size, could also be \PYGZsq{}step\PYGZsq{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epoch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The frequency of the scheduler}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{monitor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Metric for `ReduceLROnPlateau` to monitor}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Whether to crash the training if `monitor` is not found}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Custom name for `LearningRateMonitor` to use}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Only the \sphinxcode{\sphinxupquote{"scheduler"}} key is required, the rest will be set to the defaults above.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{frequency}} value specified in a dict along with the \sphinxcode{\sphinxupquote{optimizer}} key is an int corresponding
to the number of sequential batches optimized with the specific optimizer.
It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.
This is different from the \sphinxcode{\sphinxupquote{frequency}} value specified in the lr\_dict mentioned below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{optimizer\PYGZus{}one} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{optimizer\PYGZus{}two} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}one}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}two}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the \sphinxcode{\sphinxupquote{lr\_scheduler}} key in the above dict,
the scheduler will only be updated when its optimizer is being used.
\end{sphinxadmonition}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} most cases}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} multiple optimizer case (e.g.: GAN)}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}

\PYG{c+c1}{\PYGZsh{} example with learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with step\PYGZhy{}based learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{gen\PYGZus{}sch} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ExponentialLR}\PYG{p}{(}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+m+mf}{0.99}\PYG{p}{)}\PYG{p}{,}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} called after each training step}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} called every epoch}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{gen\PYGZus{}sch}\PYG{p}{,} \PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with optimizer frequencies}
\PYG{c+c1}{\PYGZsh{} see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1}
\PYG{c+c1}{\PYGZsh{} https://arxiv.org/abs/1704.00028}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}critic} \PYG{o}{=} \PYG{l+m+mi}{5}
    \PYG{k}{return} \PYG{p}{(}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{n\PYGZus{}critic}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Some things to know:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lightning calls \sphinxcode{\sphinxupquote{.backward()}} and \sphinxcode{\sphinxupquote{.step()}} on each optimizer and learning rate scheduler as needed.

\item {} 
\sphinxAtStartPar
If you use 16\sphinxhyphen{}bit precision (\sphinxcode{\sphinxupquote{precision=16}}), Lightning will automatically handle the optimizers.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.training_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{training\_step()}}}}} will have an additional \sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\item {} 
\sphinxAtStartPar
If you use \sphinxcode{\sphinxupquote{torch.optim.LBFGS}}, Lightning handles the closure function automatically for you.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
at each training step.

\item {} 
\sphinxAtStartPar
If you need to control how often those optimizers step or override the default \sphinxcode{\sphinxupquote{.step()}} schedule,
override the \sphinxcode{\sphinxupquote{optimizer\_step()}} hook.

\end{itemize}
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{forward()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
\sphinxAtStartPar
Same as \sphinxcode{\sphinxupquote{torch.nn.Module.forward()}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{*args}} \textendash{} Whatever you decide to pass into the forward method.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} \textendash{} Keyword arguments are also possible.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Your model’s output

\end{description}\end{quote}

\end{fulllineitems}

\index{on\_train\_epoch\_end() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{on\_train\_epoch\_end()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.on_train_epoch_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_end}}}{\emph{\DUrole{n}{outputs}}}{}
\sphinxAtStartPar
Called in the training loop at the very end of the epoch.

\sphinxAtStartPar
To access all batch outputs at the end of the epoch, either:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Implement \sphinxtitleref{training\_epoch\_end} in the LightningModule OR

\item {} 
\sphinxAtStartPar
Cache data across steps on the attribute(s) of the \sphinxtitleref{LightningModule} and access them in this hook

\end{enumerate}

\end{fulllineitems}

\index{on\_train\_epoch\_start() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{on\_train\_epoch\_start()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.on_train_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the training loop at the very beginning of the epoch.

\end{fulllineitems}

\index{on\_validation\_epoch\_start() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{on\_validation\_epoch\_start()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.on_validation_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_validation\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the validation loop at the very beginning of the epoch.

\end{fulllineitems}

\index{training\_step() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{training\_step()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.training_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Integer displaying index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{optimizer\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} When using multiple optimizers, this argument will also be present.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hiddens}} (\sphinxcode{\sphinxupquote{Tensor}}) \textendash{} Passed in if
{\color{red}\bfseries{}:paramref:\textasciigrave{}\textasciitilde{}pytorch\_lightning.core.lightning.LightningModule.truncated\_bptt\_steps\textasciigrave{}} \textgreater{} 0.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Tensor}} \sphinxhyphen{} The loss tensor

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dict}} \sphinxhyphen{} A dictionary. Can include any keys, but must include the key \sphinxcode{\sphinxupquote{\textquotesingle{}loss\textquotesingle{}}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Training will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Returning \sphinxcode{\sphinxupquote{None}} is currently not supported for multi\sphinxhyphen{}GPU or TPU, or with 16\sphinxhyphen{}bit precision enabled.
\end{sphinxadmonition}

\sphinxAtStartPar
In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{o}{=} \PYG{n}{batch}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{encoder}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you define multiple optimizers, this step will be called with an additional
\sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Multiple optimizers (e.g.: GANs)}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{optimizer\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with encoder}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with decoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Truncated back\PYGZhy{}propagation through time}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} hiddens are the hidden states from the previous truncated backprop step}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{n}{out}\PYG{p}{,} \PYG{n}{hiddens} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lstm}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hiddens}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{hiddens}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.
\end{sphinxadmonition}

\end{fulllineitems}

\index{validation\_step() (vipercore.ml.plmodels.AutoEncoderModel method)@\spxentry{validation\_step()}\spxextra{vipercore.ml.plmodels.AutoEncoderModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.validation_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{validation\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} the pseudocode for these calls}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{val\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{val\PYGZus{}data}\PYG{p}{:}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{val\PYGZus{}batch}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}outs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{validation\PYGZus{}epoch\PYGZus{}end}\PYG{p}{(}\PYG{n}{val\PYGZus{}outs}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataloader\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The index of the dataloader that produced this batch
(only if multiple val dataloaders used)

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Any object or value

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Validation will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} pseudocode of order}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{val\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{val\PYGZus{}data}\PYG{p}{:}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{val\PYGZus{}batch}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{defined}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{validation\PYGZus{}step\PYGZus{}end}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step\PYGZus{}end}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}outs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{n}{validation\PYGZus{}epoch\PYGZus{}end}\PYG{p}{(}\PYG{n}{val\PYGZus{}outs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if you have one val dataloader:}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} if you have multiple val dataloaders:}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{dataloader\PYGZus{}idx}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CASE 1: A single validation dataset}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{batch}

    \PYG{c+c1}{\PYGZsh{} implement your own}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} log 6 example images}
    \PYG{c+c1}{\PYGZsh{} or generated text... or whatever}
    \PYG{n}{sample\PYGZus{}imgs} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}
    \PYG{n}{grid} \PYG{o}{=} \PYG{n}{torchvision}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{make\PYGZus{}grid}\PYG{p}{(}\PYG{n}{sample\PYGZus{}imgs}\PYG{p}{)}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{logger}\PYG{o}{.}\PYG{n}{experiment}\PYG{o}{.}\PYG{n}{add\PYGZus{}image}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example\PYGZus{}images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{grid}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} calculate acc}
    \PYG{n}{labels\PYGZus{}hat} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}acc} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y} \PYG{o}{==} \PYG{n}{labels\PYGZus{}hat}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{1.0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} log the outputs!}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}dict}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}acc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{val\PYGZus{}acc}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you pass in multiple val dataloaders, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.validation_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{validation\_step()}}}}} will have an additional argument.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CASE 2: multiple validation dataloaders}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{dataloader\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} dataloader\PYGZus{}idx tells you which dataset this is.}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If you don’t need to validate you don’t need to implement this method.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
When the {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.AutoEncoderModel.validation_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{validation\_step()}}}}} is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{GeneralModel (class in vipercore.ml.plmodels)@\spxentry{GeneralModel}\spxextra{class in vipercore.ml.plmodels}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.plmodels.}}\sphinxbfcode{\sphinxupquote{GeneralModel}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{hparams}}}{}~\index{configure\_optimizers() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{configure\_optimizers()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.configure_optimizers}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{configure\_optimizers}}}{}{}
\sphinxAtStartPar
Choose what optimizers and learning\sphinxhyphen{}rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of these 6 options.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single optimizer}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{List or Tuple} of optimizers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two lists} \sphinxhyphen{} The first list has multiple optimizers, and the second has multiple LR schedulers (or
multiple lr\_dict).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dictionary}, with an \sphinxcode{\sphinxupquote{"optimizer"}} key, and (optionally) a \sphinxcode{\sphinxupquote{"lr\_scheduler"}}
key whose value is a single LR scheduler or lr\_dict.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tuple of dictionaries} as described above, with an optional \sphinxcode{\sphinxupquote{"frequency"}} key.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{None} \sphinxhyphen{} Fit will run without any optimizer.

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The lr\_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lr\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{lr\PYGZus{}scheduler}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The LR scheduler instance (required)}
    \PYG{c+c1}{\PYGZsh{} The unit of the scheduler\PYGZsq{}s step size, could also be \PYGZsq{}step\PYGZsq{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epoch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The frequency of the scheduler}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{monitor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Metric for `ReduceLROnPlateau` to monitor}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Whether to crash the training if `monitor` is not found}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Custom name for `LearningRateMonitor` to use}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Only the \sphinxcode{\sphinxupquote{"scheduler"}} key is required, the rest will be set to the defaults above.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{frequency}} value specified in a dict along with the \sphinxcode{\sphinxupquote{optimizer}} key is an int corresponding
to the number of sequential batches optimized with the specific optimizer.
It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.
This is different from the \sphinxcode{\sphinxupquote{frequency}} value specified in the lr\_dict mentioned below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{optimizer\PYGZus{}one} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{optimizer\PYGZus{}two} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}one}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}two}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the \sphinxcode{\sphinxupquote{lr\_scheduler}} key in the above dict,
the scheduler will only be updated when its optimizer is being used.
\end{sphinxadmonition}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} most cases}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} multiple optimizer case (e.g.: GAN)}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}

\PYG{c+c1}{\PYGZsh{} example with learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with step\PYGZhy{}based learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{gen\PYGZus{}sch} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ExponentialLR}\PYG{p}{(}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+m+mf}{0.99}\PYG{p}{)}\PYG{p}{,}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} called after each training step}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} called every epoch}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{gen\PYGZus{}sch}\PYG{p}{,} \PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with optimizer frequencies}
\PYG{c+c1}{\PYGZsh{} see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1}
\PYG{c+c1}{\PYGZsh{} https://arxiv.org/abs/1704.00028}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}critic} \PYG{o}{=} \PYG{l+m+mi}{5}
    \PYG{k}{return} \PYG{p}{(}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{n\PYGZus{}critic}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Some things to know:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lightning calls \sphinxcode{\sphinxupquote{.backward()}} and \sphinxcode{\sphinxupquote{.step()}} on each optimizer and learning rate scheduler as needed.

\item {} 
\sphinxAtStartPar
If you use 16\sphinxhyphen{}bit precision (\sphinxcode{\sphinxupquote{precision=16}}), Lightning will automatically handle the optimizers.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.training_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{training\_step()}}}}} will have an additional \sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\item {} 
\sphinxAtStartPar
If you use \sphinxcode{\sphinxupquote{torch.optim.LBFGS}}, Lightning handles the closure function automatically for you.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
at each training step.

\item {} 
\sphinxAtStartPar
If you need to control how often those optimizers step or override the default \sphinxcode{\sphinxupquote{.step()}} schedule,
override the \sphinxcode{\sphinxupquote{optimizer\_step()}} hook.

\end{itemize}
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{forward()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
\sphinxAtStartPar
Same as \sphinxcode{\sphinxupquote{torch.nn.Module.forward()}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{*args}} \textendash{} Whatever you decide to pass into the forward method.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} \textendash{} Keyword arguments are also possible.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Your model’s output

\end{description}\end{quote}

\end{fulllineitems}

\index{on\_train\_epoch\_end() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{on\_train\_epoch\_end()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.on_train_epoch_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_end}}}{\emph{\DUrole{n}{outputs}}}{}
\sphinxAtStartPar
Called in the training loop at the very end of the epoch.

\sphinxAtStartPar
To access all batch outputs at the end of the epoch, either:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Implement \sphinxtitleref{training\_epoch\_end} in the LightningModule OR

\item {} 
\sphinxAtStartPar
Cache data across steps on the attribute(s) of the \sphinxtitleref{LightningModule} and access them in this hook

\end{enumerate}

\end{fulllineitems}

\index{on\_train\_epoch\_start() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{on\_train\_epoch\_start()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.on_train_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the training loop at the very beginning of the epoch.

\end{fulllineitems}

\index{on\_train\_start() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{on\_train\_start()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.on_train_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_start}}}{}{}
\sphinxAtStartPar
Called at the beginning of training after sanity check.

\end{fulllineitems}

\index{on\_validation\_epoch\_end() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{on\_validation\_epoch\_end()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.on_validation_epoch_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_validation\_epoch\_end}}}{}{}
\sphinxAtStartPar
Called in the validation loop at the very end of the epoch.

\end{fulllineitems}

\index{on\_validation\_epoch\_start() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{on\_validation\_epoch\_start()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.on_validation_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_validation\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the validation loop at the very beginning of the epoch.

\end{fulllineitems}

\index{training\_step() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{training\_step()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.training_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Integer displaying index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{optimizer\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} When using multiple optimizers, this argument will also be present.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hiddens}} (\sphinxcode{\sphinxupquote{Tensor}}) \textendash{} Passed in if
{\color{red}\bfseries{}:paramref:\textasciigrave{}\textasciitilde{}pytorch\_lightning.core.lightning.LightningModule.truncated\_bptt\_steps\textasciigrave{}} \textgreater{} 0.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Tensor}} \sphinxhyphen{} The loss tensor

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dict}} \sphinxhyphen{} A dictionary. Can include any keys, but must include the key \sphinxcode{\sphinxupquote{\textquotesingle{}loss\textquotesingle{}}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Training will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Returning \sphinxcode{\sphinxupquote{None}} is currently not supported for multi\sphinxhyphen{}GPU or TPU, or with 16\sphinxhyphen{}bit precision enabled.
\end{sphinxadmonition}

\sphinxAtStartPar
In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{o}{=} \PYG{n}{batch}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{encoder}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you define multiple optimizers, this step will be called with an additional
\sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Multiple optimizers (e.g.: GANs)}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{optimizer\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with encoder}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with decoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Truncated back\PYGZhy{}propagation through time}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} hiddens are the hidden states from the previous truncated backprop step}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{n}{out}\PYG{p}{,} \PYG{n}{hiddens} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lstm}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hiddens}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{hiddens}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.
\end{sphinxadmonition}

\end{fulllineitems}

\index{validation\_step() (vipercore.ml.plmodels.GeneralModel method)@\spxentry{validation\_step()}\spxextra{vipercore.ml.plmodels.GeneralModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.validation_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{validation\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} the pseudocode for these calls}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{val\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{val\PYGZus{}data}\PYG{p}{:}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{val\PYGZus{}batch}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}outs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{validation\PYGZus{}epoch\PYGZus{}end}\PYG{p}{(}\PYG{n}{val\PYGZus{}outs}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataloader\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The index of the dataloader that produced this batch
(only if multiple val dataloaders used)

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Any object or value

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Validation will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} pseudocode of order}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{val\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{val\PYGZus{}data}\PYG{p}{:}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{val\PYGZus{}batch}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{defined}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{validation\PYGZus{}step\PYGZus{}end}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step\PYGZus{}end}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}outs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{n}{validation\PYGZus{}epoch\PYGZus{}end}\PYG{p}{(}\PYG{n}{val\PYGZus{}outs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if you have one val dataloader:}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} if you have multiple val dataloaders:}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{dataloader\PYGZus{}idx}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CASE 1: A single validation dataset}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{batch}

    \PYG{c+c1}{\PYGZsh{} implement your own}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} log 6 example images}
    \PYG{c+c1}{\PYGZsh{} or generated text... or whatever}
    \PYG{n}{sample\PYGZus{}imgs} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}
    \PYG{n}{grid} \PYG{o}{=} \PYG{n}{torchvision}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{make\PYGZus{}grid}\PYG{p}{(}\PYG{n}{sample\PYGZus{}imgs}\PYG{p}{)}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{logger}\PYG{o}{.}\PYG{n}{experiment}\PYG{o}{.}\PYG{n}{add\PYGZus{}image}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example\PYGZus{}images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{grid}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} calculate acc}
    \PYG{n}{labels\PYGZus{}hat} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}acc} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y} \PYG{o}{==} \PYG{n}{labels\PYGZus{}hat}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{1.0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} log the outputs!}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}dict}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}acc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{val\PYGZus{}acc}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you pass in multiple val dataloaders, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.validation_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{validation\_step()}}}}} will have an additional argument.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CASE 2: multiple validation dataloaders}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{dataloader\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} dataloader\PYGZus{}idx tells you which dataset this is.}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If you don’t need to validate you don’t need to implement this method.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
When the {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.GeneralModel.validation_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{validation\_step()}}}}} is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{MultilabelSupervisedModel (class in vipercore.ml.plmodels)@\spxentry{MultilabelSupervisedModel}\spxextra{class in vipercore.ml.plmodels}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{vipercore.ml.plmodels.}}\sphinxbfcode{\sphinxupquote{MultilabelSupervisedModel}}}{\emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{configure\_optimizers() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{configure\_optimizers()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.configure_optimizers}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{configure\_optimizers}}}{}{}
\sphinxAtStartPar
Choose what optimizers and learning\sphinxhyphen{}rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of these 6 options.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single optimizer}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{List or Tuple} of optimizers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two lists} \sphinxhyphen{} The first list has multiple optimizers, and the second has multiple LR schedulers (or
multiple lr\_dict).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dictionary}, with an \sphinxcode{\sphinxupquote{"optimizer"}} key, and (optionally) a \sphinxcode{\sphinxupquote{"lr\_scheduler"}}
key whose value is a single LR scheduler or lr\_dict.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tuple of dictionaries} as described above, with an optional \sphinxcode{\sphinxupquote{"frequency"}} key.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{None} \sphinxhyphen{} Fit will run without any optimizer.

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The lr\_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lr\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{lr\PYGZus{}scheduler}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The LR scheduler instance (required)}
    \PYG{c+c1}{\PYGZsh{} The unit of the scheduler\PYGZsq{}s step size, could also be \PYGZsq{}step\PYGZsq{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epoch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} The frequency of the scheduler}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{monitor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Metric for `ReduceLROnPlateau` to monitor}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Whether to crash the training if `monitor` is not found}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} Custom name for `LearningRateMonitor` to use}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Only the \sphinxcode{\sphinxupquote{"scheduler"}} key is required, the rest will be set to the defaults above.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{frequency}} value specified in a dict along with the \sphinxcode{\sphinxupquote{optimizer}} key is an int corresponding
to the number of sequential batches optimized with the specific optimizer.
It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.
This is different from the \sphinxcode{\sphinxupquote{frequency}} value specified in the lr\_dict mentioned below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{optimizer\PYGZus{}one} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{optimizer\PYGZus{}two} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}one}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{optimizer\PYGZus{}two}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the \sphinxcode{\sphinxupquote{lr\_scheduler}} key in the above dict,
the scheduler will only be updated when its optimizer is being used.
\end{sphinxadmonition}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} most cases}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} multiple optimizer case (e.g.: GAN)}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}

\PYG{c+c1}{\PYGZsh{} example with learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with step\PYGZhy{}based learning rate schedulers}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{gen\PYGZus{}sch} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scheduler}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ExponentialLR}\PYG{p}{(}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+m+mf}{0.99}\PYG{p}{)}\PYG{p}{,}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} called after each training step}
    \PYG{n}{dis\PYGZus{}sch} \PYG{o}{=} \PYG{n}{CosineAnnealing}\PYG{p}{(}\PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{n}{T\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} called every epoch}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{gen\PYGZus{}sch}\PYG{p}{,} \PYG{n}{dis\PYGZus{}sch}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} example with optimizer frequencies}
\PYG{c+c1}{\PYGZsh{} see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1}
\PYG{c+c1}{\PYGZsh{} https://arxiv.org/abs/1704.00028}
\PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{gen\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}gen}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
    \PYG{n}{dis\PYGZus{}opt} \PYG{o}{=} \PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model\PYGZus{}dis}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}critic} \PYG{o}{=} \PYG{l+m+mi}{5}
    \PYG{k}{return} \PYG{p}{(}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{dis\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{n\PYGZus{}critic}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{gen\PYGZus{}opt}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Some things to know:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lightning calls \sphinxcode{\sphinxupquote{.backward()}} and \sphinxcode{\sphinxupquote{.step()}} on each optimizer and learning rate scheduler as needed.

\item {} 
\sphinxAtStartPar
If you use 16\sphinxhyphen{}bit precision (\sphinxcode{\sphinxupquote{precision=16}}), Lightning will automatically handle the optimizers.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.training_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{training\_step()}}}}} will have an additional \sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\item {} 
\sphinxAtStartPar
If you use \sphinxcode{\sphinxupquote{torch.optim.LBFGS}}, Lightning handles the closure function automatically for you.

\item {} 
\sphinxAtStartPar
If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
at each training step.

\item {} 
\sphinxAtStartPar
If you need to control how often those optimizers step or override the default \sphinxcode{\sphinxupquote{.step()}} schedule,
override the \sphinxcode{\sphinxupquote{optimizer\_step()}} hook.

\end{itemize}
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{forward()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
\sphinxAtStartPar
Same as \sphinxcode{\sphinxupquote{torch.nn.Module.forward()}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{*args}} \textendash{} Whatever you decide to pass into the forward method.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} \textendash{} Keyword arguments are also possible.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Your model’s output

\end{description}\end{quote}

\end{fulllineitems}

\index{on\_train\_epoch\_end() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{on\_train\_epoch\_end()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.on_train_epoch_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_end}}}{\emph{\DUrole{n}{outputs}}}{}
\sphinxAtStartPar
Called in the training loop at the very end of the epoch.

\sphinxAtStartPar
To access all batch outputs at the end of the epoch, either:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Implement \sphinxtitleref{training\_epoch\_end} in the LightningModule OR

\item {} 
\sphinxAtStartPar
Cache data across steps on the attribute(s) of the \sphinxtitleref{LightningModule} and access them in this hook

\end{enumerate}

\end{fulllineitems}

\index{on\_train\_epoch\_start() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{on\_train\_epoch\_start()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.on_train_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the training loop at the very beginning of the epoch.

\end{fulllineitems}

\index{on\_train\_start() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{on\_train\_start()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.on_train_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_train\_start}}}{}{}
\sphinxAtStartPar
Called at the beginning of training after sanity check.

\end{fulllineitems}

\index{on\_validation\_epoch\_end() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{on\_validation\_epoch\_end()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.on_validation_epoch_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_validation\_epoch\_end}}}{}{}
\sphinxAtStartPar
Called in the validation loop at the very end of the epoch.

\end{fulllineitems}

\index{on\_validation\_epoch\_start() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{on\_validation\_epoch\_start()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.on_validation_epoch_start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_validation\_epoch\_start}}}{}{}
\sphinxAtStartPar
Called in the validation loop at the very beginning of the epoch.

\end{fulllineitems}

\index{training\_step() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{training\_step()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.training_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Integer displaying index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{optimizer\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} When using multiple optimizers, this argument will also be present.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hiddens}} (\sphinxcode{\sphinxupquote{Tensor}}) \textendash{} Passed in if
{\color{red}\bfseries{}:paramref:\textasciigrave{}\textasciitilde{}pytorch\_lightning.core.lightning.LightningModule.truncated\_bptt\_steps\textasciigrave{}} \textgreater{} 0.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Tensor}} \sphinxhyphen{} The loss tensor

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dict}} \sphinxhyphen{} A dictionary. Can include any keys, but must include the key \sphinxcode{\sphinxupquote{\textquotesingle{}loss\textquotesingle{}}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Training will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Returning \sphinxcode{\sphinxupquote{None}} is currently not supported for multi\sphinxhyphen{}GPU or TPU, or with 16\sphinxhyphen{}bit precision enabled.
\end{sphinxadmonition}

\sphinxAtStartPar
In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.

\sphinxAtStartPar
Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{o}{=} \PYG{n}{batch}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{encoder}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you define multiple optimizers, this step will be called with an additional
\sphinxcode{\sphinxupquote{optimizer\_idx}} parameter.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Multiple optimizers (e.g.: GANs)}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{optimizer\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with encoder}
    \PYG{k}{if} \PYG{n}{optimizer\PYGZus{}idx} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} do training\PYGZus{}step with decoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Truncated back\PYGZhy{}propagation through time}
\PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} hiddens are the hidden states from the previous truncated backprop step}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{n}{out}\PYG{p}{,} \PYG{n}{hiddens} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lstm}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{hiddens}\PYG{p}{)}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
    \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hiddens}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{hiddens}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.
\end{sphinxadmonition}

\end{fulllineitems}

\index{validation\_step() (vipercore.ml.plmodels.MultilabelSupervisedModel method)@\spxentry{validation\_step()}\spxextra{vipercore.ml.plmodels.MultilabelSupervisedModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.validation_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{validation\_step}}}{\emph{\DUrole{n}{batch}}, \emph{\DUrole{n}{batch\_idx}}}{}
\sphinxAtStartPar
Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} the pseudocode for these calls}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{val\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{val\PYGZus{}data}\PYG{p}{:}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{val\PYGZus{}batch}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}outs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{validation\PYGZus{}epoch\PYGZus{}end}\PYG{p}{(}\PYG{n}{val\PYGZus{}outs}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch}} (\sphinxcode{\sphinxupquote{Tensor}} | (\sphinxcode{\sphinxupquote{Tensor}}, …) | {[}\sphinxcode{\sphinxupquote{Tensor}}, …{]}) \textendash{} The output of your \sphinxcode{\sphinxupquote{DataLoader}}. A tensor, tuple or list.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The index of this batch

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataloader\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The index of the dataloader that produced this batch
(only if multiple val dataloaders used)

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar

\sphinxAtStartPar
Any of.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Any object or value

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{None}} \sphinxhyphen{} Validation will skip to the next batch

\end{itemize}


\end{description}\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} pseudocode of order}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{val\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{val\PYGZus{}data}\PYG{p}{:}
    \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{val\PYGZus{}batch}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{defined}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{validation\PYGZus{}step\PYGZus{}end}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{validation\PYGZus{}step\PYGZus{}end}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}outs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{val\PYGZus{}outs} \PYG{o}{=} \PYG{n}{validation\PYGZus{}epoch\PYGZus{}end}\PYG{p}{(}\PYG{n}{val\PYGZus{}outs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} if you have one val dataloader:}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} if you have multiple val dataloaders:}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{dataloader\PYGZus{}idx}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CASE 1: A single validation dataset}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{batch}

    \PYG{c+c1}{\PYGZsh{} implement your own}
    \PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} log 6 example images}
    \PYG{c+c1}{\PYGZsh{} or generated text... or whatever}
    \PYG{n}{sample\PYGZus{}imgs} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}
    \PYG{n}{grid} \PYG{o}{=} \PYG{n}{torchvision}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{make\PYGZus{}grid}\PYG{p}{(}\PYG{n}{sample\PYGZus{}imgs}\PYG{p}{)}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{logger}\PYG{o}{.}\PYG{n}{experiment}\PYG{o}{.}\PYG{n}{add\PYGZus{}image}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example\PYGZus{}images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{grid}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} calculate acc}
    \PYG{n}{labels\PYGZus{}hat} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{out}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{val\PYGZus{}acc} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y} \PYG{o}{==} \PYG{n}{labels\PYGZus{}hat}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{1.0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} log the outputs!}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}dict}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{loss}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val\PYGZus{}acc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{val\PYGZus{}acc}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If you pass in multiple val dataloaders, {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.validation_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{validation\_step()}}}}} will have an additional argument.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CASE 2: multiple validation dataloaders}
\PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{dataloader\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} dataloader\PYGZus{}idx tells you which dataset this is.}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If you don’t need to validate you don’t need to implement this method.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
When the {\hyperref[\detokenize{pages/ml:vipercore.ml.plmodels.MultilabelSupervisedModel.validation_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{validation\_step()}}}}} is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{v}
\item\relax\sphinxstyleindexentry{vipercore.ml.datasets}\sphinxstyleindexpageref{pages/ml:\detokenize{module-vipercore.ml.datasets}}
\item\relax\sphinxstyleindexentry{vipercore.ml.metrics}\sphinxstyleindexpageref{pages/ml:\detokenize{module-vipercore.ml.metrics}}
\item\relax\sphinxstyleindexentry{vipercore.ml.models}\sphinxstyleindexpageref{pages/ml:\detokenize{module-vipercore.ml.models}}
\item\relax\sphinxstyleindexentry{vipercore.ml.plmodels}\sphinxstyleindexpageref{pages/ml:\detokenize{module-vipercore.ml.plmodels}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}